> !!! Be careful those description for the majority have not been rephrased !!!

[ConceptNet], [ATOMIC], [GLUCOSE]

## Outsiders 

## NLU 

[[GLUE]] : The [General Language Understanding Evaluation](https://arxiv.org/pdf/1804.07461.pdf) benchmark (GLUE) is a collection of datasets used for training, evaluating, and analyzing NLP models relative to one another, with the goal of driving “research in the development of general and robust natural language understanding systems.” The collection consists of nine “difficult and diverse” task datasets designed to test a model’s language understanding
NER - Coref - Entailement

[[SuperGLUE]] : hardest task than glue 

## KB 

#### Population :

[[Perlex]] [[FarsBase-KBP]] persian english dataset 

[[ConceptNet]]
[[BabalNet]]


## RE / RC 

[[TRACED]] : The TAC Relation Extraction Dataset (TACRED) is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. These examples are created by combining available human annotations from the TAC KBP challenges and crowdsourcing. Stanford paper [here](https://nlp.stanford.edu/pubs/zhang2017tacred.pdf)

## WSD 

[[SemCor]] corpus is an English corpus with semantically annotated texts. The semantic analysis was done manually with WordNet 1.6 senses (SemCor version 1.6) and later automatically mapped to WordNet 3.0 (SemCor version 3.0). The SemCorpus corpus consists of 352 texts from [Brown corpus](https://www.sketchengine.eu/brown-corpus/).

[[WiC]] : Word In Context , based on annotations curated by experts, for generic evaluation of context-sensitive representations. find [here]([WiC: The Word-in-Context Dataset (pilehvar.github.io)](https://pilehvar.github.io/wic/))




## Entailment / NLI 

good source [here](https://analyticsindiamag.com/most-popular-datasets-for-neural-textual-entailment-with-implementation-in-pytorch-and-tensorflow/)

[[SNLI]] : 570000 sentences sets that can either be :  entailment, contradiction and neutral (often [[RoBerta]] for this task )

[[Multi-NLI]] sentences explained 

![[Pasted image 20230930183417.png]] from [[Textual Entailment for Event Argument Extraction]]: Zero- and Few-Shot with Multi-Source Learning


## QA 

[[SQaD]] :

## Classification 

## Translation 

evaluation see [[BLEU]]

[[ACL WMT ’14]] WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words

[[BabalNet]]


## NER 

[[CoNLL-2003]]  is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition. The data consists of eight files covering two languages: English and German. For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.
